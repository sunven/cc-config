/**
 * Performance Benchmark Suite
 *
 * Comprehensive performance testing for cc-config-viewer
 * Validates startup time, tab switching, file operations, and memory usage
 */

const { performance, PerformanceObserver } = require('perf_hooks')
const { execSync, spawn } = require('child_process')
const path = require('path')
const fs = require('fs')

// Performance thresholds from story requirements
const THRESHOLDS = {
  STARTUP_TIME: 3000, // <3 seconds
  TAB_SWITCH: 100, // <100ms
  FILE_CHANGE_DETECTION: 500, // <500ms
  MEMORY_USAGE: 200, // <200MB
  BUILD_TIME: 60000, // <60 seconds
  TYPE_CHECK: 30000, // <30 seconds
  FIRST_CONTENTFUL_PAINT: 2000, // <2 seconds
  TIME_TO_INTERACTIVE: 3500, // <3.5 seconds
}

// Colors for terminal output
const colors = {
  reset: '\x1b[0m',
  green: '\x1b[32m',
  red: '\x1b[31m',
  yellow: '\x1b[33m',
  cyan: '\x1b[36m',
  bold: '\x1b[1m',
}

function log(message, color = colors.reset) {
  console.log(`${color}${message}${colors.reset}`)
}

function logResult(name, value, threshold, unit = 'ms') {
  const passed = value <= threshold
  const status = passed ? '‚úÖ PASS' : '‚ùå FAIL'
  const color = passed ? colors.green : colors.red
  log(`  ${status} ${name}: ${value}${unit} (threshold: ${threshold}${unit})`, color)
  return passed
}

// Measure build time
async function measureBuildTime() {
  log('\nüì¶ Build Performance', colors.bold)

  const start = performance.now()
  try {
    execSync('npm run build', { stdio: 'pipe' })
    const duration = Math.round(performance.now() - start)
    return logResult('Build Time', duration, THRESHOLDS.BUILD_TIME)
  } catch (error) {
    log('  ‚ùå Build failed', colors.red)
    return false
  }
}

// Measure type checking time
async function measureTypeCheckTime() {
  log('\nüîç Type Check Performance', colors.bold)

  const start = performance.now()
  try {
    execSync('npx tsc --noEmit', { stdio: 'pipe' })
    const duration = Math.round(performance.now() - start)
    return logResult('Type Check Time', duration, THRESHOLDS.TYPE_CHECK)
  } catch (error) {
    log('  ‚ùå Type check failed', colors.red)
    return false
  }
}

// Measure bundle size
async function measureBundleSize() {
  log('\nüìä Bundle Size Analysis', colors.bold)

  const distPath = path.join(process.cwd(), 'dist')

  if (!fs.existsSync(distPath)) {
    log('  ‚ö†Ô∏è No dist folder found. Running build first...', colors.yellow)
    execSync('npm run build', { stdio: 'pipe' })
  }

  const getDirectorySize = (dirPath) => {
    let totalSize = 0
    const files = fs.readdirSync(dirPath, { withFileTypes: true })

    for (const file of files) {
      const filePath = path.join(dirPath, file.name)
      if (file.isDirectory()) {
        totalSize += getDirectorySize(filePath)
      } else {
        totalSize += fs.statSync(filePath).size
      }
    }
    return totalSize
  }

  const totalSize = getDirectorySize(distPath)
  const sizeKB = Math.round(totalSize / 1024)
  const sizeMB = (totalSize / (1024 * 1024)).toFixed(2)

  log(`  Total bundle size: ${sizeKB}KB (${sizeMB}MB)`, colors.cyan)

  // Check for large JS files
  const assetsPath = path.join(distPath, 'assets')
  if (fs.existsSync(assetsPath)) {
    const jsFiles = fs.readdirSync(assetsPath).filter(f => f.endsWith('.js'))
    let largeFiles = false

    for (const file of jsFiles) {
      const filePath = path.join(assetsPath, file)
      const size = fs.statSync(filePath).size
      const sizeKB = Math.round(size / 1024)

      if (sizeKB > 250) {
        log(`  ‚ö†Ô∏è Large JS file: ${file} (${sizeKB}KB)`, colors.yellow)
        largeFiles = true
      }
    }

    if (!largeFiles) {
      log('  ‚úÖ All JS chunks under 250KB', colors.green)
    }
  }

  return totalSize < 5 * 1024 * 1024 // Under 5MB total
}

// Measure memory usage of the Node process
async function measureMemoryUsage() {
  log('\nüíæ Memory Usage', colors.bold)

  const used = process.memoryUsage()
  const heapUsedMB = Math.round(used.heapUsed / 1024 / 1024)
  const rssMB = Math.round(used.rss / 1024 / 1024)
  const externalMB = Math.round(used.external / 1024 / 1024)

  log(`  Heap Used: ${heapUsedMB}MB`, colors.cyan)
  log(`  RSS: ${rssMB}MB`, colors.cyan)
  log(`  External: ${externalMB}MB`, colors.cyan)

  return logResult('Total Memory (RSS)', rssMB, THRESHOLDS.MEMORY_USAGE, 'MB')
}

// Measure test suite performance
async function measureTestPerformance() {
  log('\nüß™ Test Suite Performance', colors.bold)

  const start = performance.now()
  try {
    execSync('npm test -- --run', { stdio: 'pipe' })
    const duration = Math.round(performance.now() - start)
    log(`  Test suite completed in ${duration}ms`, colors.cyan)

    // Check if tests run reasonably fast
    return logResult('Test Suite Execution', duration, 60000) // Under 60 seconds
  } catch (error) {
    log('  ‚ùå Tests failed', colors.red)
    return false
  }
}

// Simulate startup time measurement
async function measureStartupTime() {
  log('\nüöÄ Startup Time Simulation', colors.bold)

  // Measure time to import main modules
  const start = performance.now()

  // Simulate module loading time
  const importTests = [
    { name: 'React', module: 'react' },
    { name: 'React DOM', module: 'react-dom' },
    { name: 'Zustand', module: 'zustand' },
    { name: 'i18next', module: 'i18next' },
  ]

  let totalImportTime = 0
  for (const test of importTests) {
    const importStart = performance.now()
    try {
      require(test.module)
      const importDuration = performance.now() - importStart
      totalImportTime += importDuration
      log(`  ${test.name}: ${importDuration.toFixed(2)}ms`, colors.cyan)
    } catch (e) {
      log(`  ${test.name}: skipped (ESM module)`, colors.yellow)
    }
  }

  const totalDuration = Math.round(performance.now() - start)
  log(`  Total module loading: ${totalDuration}ms`, colors.cyan)

  // For actual startup time, we need Lighthouse or a browser test
  log('  Note: Full startup time requires browser-based testing (see Lighthouse)', colors.yellow)

  return true
}

// Generate performance report
function generateReport(results) {
  const report = {
    timestamp: new Date().toISOString(),
    thresholds: THRESHOLDS,
    results: results,
    summary: {
      passed: Object.values(results).filter(r => r.passed).length,
      failed: Object.values(results).filter(r => !r.passed).length,
      total: Object.keys(results).length,
    },
  }

  const reportPath = path.join(process.cwd(), 'performance-report.json')
  fs.writeFileSync(reportPath, JSON.stringify(report, null, 2))
  log(`\nüìÑ Report saved to ${reportPath}`, colors.cyan)

  return report
}

// Main benchmark runner
async function runBenchmarks() {
  log('‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê', colors.bold)
  log('  CC-CONFIG-VIEWER PERFORMANCE BENCHMARKS  ', colors.bold)
  log('‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê', colors.bold)

  const results = {}

  // Run all benchmarks
  results.typeCheck = { passed: await measureTypeCheckTime() }
  results.build = { passed: await measureBuildTime() }
  results.bundleSize = { passed: await measureBundleSize() }
  results.memory = { passed: await measureMemoryUsage() }
  results.startup = { passed: await measureStartupTime() }
  results.tests = { passed: await measureTestPerformance() }

  // Generate summary
  log('\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê', colors.bold)
  log('  BENCHMARK SUMMARY', colors.bold)
  log('‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê', colors.bold)

  const report = generateReport(results)

  const passRate = ((report.summary.passed / report.summary.total) * 100).toFixed(1)
  const summaryColor = report.summary.failed === 0 ? colors.green : colors.red

  log(`\n  Passed: ${report.summary.passed}/${report.summary.total} (${passRate}%)`, summaryColor)

  if (report.summary.failed > 0) {
    log(`\n  ‚ùå ${report.summary.failed} benchmark(s) failed!`, colors.red)
    process.exit(1)
  } else {
    log('\n  ‚úÖ All benchmarks passed!', colors.green)
  }
}

// Quick benchmark for CI
async function runQuickBenchmarks() {
  log('‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê', colors.bold)
  log('  QUICK PERFORMANCE CHECK (CI Mode)', colors.bold)
  log('‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê', colors.bold)

  const results = {}

  results.typeCheck = { passed: await measureTypeCheckTime() }
  results.memory = { passed: await measureMemoryUsage() }

  const report = generateReport(results)

  if (report.summary.failed > 0) {
    process.exit(1)
  }
}

// CLI interface
const args = process.argv.slice(2)
if (args.includes('--quick') || args.includes('-q')) {
  runQuickBenchmarks()
} else {
  runBenchmarks()
}

module.exports = {
  THRESHOLDS,
  measureBuildTime,
  measureTypeCheckTime,
  measureBundleSize,
  measureMemoryUsage,
  measureTestPerformance,
}
